{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho 1 - Coleta, Preparação e Análise de Dados\n",
    "\n",
    "## Webscrapping em servidor local\n",
    "\n",
    "### Objetivos da tarefa:\n",
    "\n",
    "- Faça um crawler capaz de navegar por todas as páginas de países e baixar seus\n",
    "  HTMLS.\n",
    "- Faça scraping dos htmls baixados e armazene os seguintes dados dos países em\n",
    "  um arquivo CSV:\n",
    "\n",
    "      1- Nome do país (campo country)\n",
    "\n",
    "      2- Nome da capital do país (campo capital)\n",
    "\n",
    "      3- Nome da moeda do país (campo Currency Name)\n",
    "\n",
    "      4- Nome do Continente (Atenção: é o nome do continente e não a sigla!)\n",
    "\n",
    "      Salvar uma coluna extra no csv contendo um timestamp do momento no qual os dados foram obtidos.\n",
    "\n",
    "- Faça um crawler que monitore as páginas de países e procure por atualizações.\n",
    "  Caso algum registro tenha sido atualizado esse deve ser atualizado no arquivo CSV, caso\n",
    "  contrário manter a versão anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1: baixando as páginas html dos países\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acessando a página principal\n",
    "url = \"http://127.0.0.1:8000/places\"\n",
    "requisicao = requests.get(url)\n",
    "\n",
    "hrefs = []\n",
    "if requisicao.status_code == 200:  # verificação de que a requisição foi bem sucedida\n",
    "    soup = BeautifulSoup(requisicao.text, 'html.parser')\n",
    "    for i in range(25): # passa por todas as páginas de index e acessa o link de cada uma\n",
    "        page_url = f\"{url}/default/index/{i}\"\n",
    "        page_requisicao = requests.get(page_url)\n",
    "        page_soup = BeautifulSoup(page_requisicao.text, 'html.parser')\n",
    "        # recuperando os links das páginas individuais dos países\n",
    "        links = page_soup.find_all('a', href=re.compile(r\"/places/default/view/.*-\\d+\"))\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            hrefs.append(href)\n",
    "    \n",
    "    # salvando os links das páginas em uma pasta, criando se já não existir\n",
    "    if not os.path.exists('html_pages'):\n",
    "        os.makedirs('html_pages')\n",
    "    for href in hrefs:\n",
    "        full_url = f\"http://127.0.0.1:8000{href}\"\n",
    "        page_response = requests.get(full_url)\n",
    "        if page_response.status_code == 200:\n",
    "            file_name = href.split('/')[-1] + '.html'\n",
    "            file_path = os.path.join('html_pages', file_name)\n",
    "            with open(file_path, 'a', encoding='utf-8') as file:\n",
    "                file.write(page_response.text)\n",
    "else:\n",
    "    print(f\"Erro: {requisicao.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2: salvando os dados da página para um csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_dir = 'html_pages'\n",
    "csv_file = 'places.csv'\n",
    "\n",
    "# mapa para converter a abreviação do continente para o nome completo\n",
    "continent_mapping = {\n",
    "    'AF': 'Africa',\n",
    "    'AN': 'Antarctica',\n",
    "    'AS': 'Asia',\n",
    "    'EU': 'Europe',\n",
    "    'NA': 'North America',\n",
    "    'OC': 'Oceania',\n",
    "    'SA': 'South America'\n",
    "}\n",
    "\n",
    "# inicializando o csv\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Country', 'Capital', 'Currency Name', 'Continent', 'Timestamp'])\n",
    "\n",
    "# itera por cada documento do diretório\n",
    "for filename in os.listdir(html_dir):\n",
    "    if filename.endswith('.html'):\n",
    "        filepath = os.path.join(html_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as html_file:\n",
    "            soup = BeautifulSoup(html_file, 'html.parser')\n",
    "            \n",
    "            # procurando os dados necessários\n",
    "            country = soup.find(id='places_country__row').find(class_='w2p_fw').text\n",
    "            capital = soup.find(id='places_capital__row').find(class_='w2p_fw').text\n",
    "            currency_name = soup.find(id='places_currency_name__row').find(class_='w2p_fw').text\n",
    "            continent_abbr = soup.find(id='places_continent__row').find(class_='w2p_fw').find('a').text\n",
    "            continent = continent_mapping.get(continent_abbr, continent_abbr)  # Convert to full name\n",
    "            \n",
    "            # pegar o horário da coleta\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # escrevendo num arquivo csv\n",
    "            with open(csv_file, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([country, capital, currency_name, continent, timestamp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: crawler que monitora a aplicação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"places.csv\", \"r\") as arq:\n",
    "    data = list(csv.reader(arq, delimiter=\";\"))\n",
    "    comparison = False\n",
    "    i = 0\n",
    "\n",
    "    for line in data[1:]:\n",
    "\n",
    "        l = line[0].split(',')\n",
    "        requisicao = requests.get(f\"http://127.0.0.1:8000{hrefs[i]}\")\n",
    "        soup = BeautifulSoup(requisicao.content, 'html.parser')\n",
    "        country = soup.find(\"tr\",{\"id\":\"places_country__row\"}).find(\"td\",{\"class\":\"w2p_fw\"}).text\n",
    "        capital = soup.find(\"tr\",{\"id\":\"places_capital__row\"}).find(\"td\",{\"class\":\"w2p_fw\"}).text\n",
    "        currency_name = soup.find(\"tr\",{\"id\":\"places_currency_name__row\"}).find(\"td\",{\"class\":\"w2p_fw\"}).text\n",
    "        continent = continent_dict[soup.find(\"tr\",{\"id\":\"places_continent__row\"}).find(\"td\",{\"class\":\"w2p_fw\"}).text]\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        if country != l[0] or capital != l[1] or currency_name != l[2] or continent != l[3]:\n",
    "            comparison = True\n",
    "            data[i] = [country,capital,currency_name, continent, timestamp]\n",
    "        i += 1\n",
    "\n",
    "    if comparison == True:\n",
    "        with open('places.csv', 'w',newline=\"\") as new:\n",
    "            writer = csv.writer(new)\n",
    "            writer.writerow(['Country', 'Capital', 'Currency Name', 'Continent', 'Timestamp'])\n",
    "            writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
